{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ae8138-780d-4880-b1d0-ae7b1326e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from utils import compute_iou\n",
    "from data.cbis_ddsm import CBIS_DDSM, get_loaders\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076940ab-fc9a-45a5-b45f-793473228fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device name NVIDIA GeForce GTX 1080 Ti\n",
      "device cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if gpu is available\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "  raise Exception(\"GPU not availalbe. CPU training will be too slow.\")\n",
    "\n",
    "print(\"device name\", torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device', device)\n",
    "\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef3ee83-19f8-49ef-83d3-c0765925f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(metrics, epoch_samples):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"Metrics: {}\".format(\", \".join(outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa8fea9-9578-4a71-a680-1f5f3d7aac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(outputs, target, metrics, bce_weight=0.5):\n",
    "    # bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    pred = torch.sigmoid(outputs)\n",
    "    # dice = dice_loss(pred, target)\n",
    "\n",
    "    # loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    # metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97c977b8-4d90-4a18-992f-e6298539fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, testloader):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(testloader)):\n",
    "\n",
    "            inputs, target = batch['image'], batch['target']\n",
    "            # mask = mask.mean(1,keepdim=True)  #transform from [batch_size, 3, size, size] to [batch,size, 1, size, size]\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    print(f'Test Acc: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f471ab6-848f-4f3c-b894-ec995744b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "num_class = 2 #binary problem  \n",
    "# model = ResNetUNet(num_class).to(device)\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model.fc = nn.Linear(num_ftrs, num_class)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# freeze backbone layers\n",
    "# for l in model.base_layers:\n",
    "#     for param in l.parameters():\n",
    "#         param.requires_grad = False\n",
    "        \n",
    "        \n",
    "# optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "\n",
    "#dataloader parameters\n",
    "\n",
    "args={\n",
    "    'data_path': \"/datasets/mamografia/CBIS-DDSM_organized/images/preprocessed\",\n",
    "    \"batch_size\": 64,\n",
    "    \"size1\": 220,\n",
    "    \"size\": 200\n",
    "}\n",
    "\n",
    "\n",
    "train_loader, test_loader = get_loaders(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e2dff-5cab-4529-af7e-ab3f034e18c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "Epoch 0 - Training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:37<11:46, 37.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:40<02:42,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     2] loss: 0.692\n",
      "[1,     3] loss: 0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:41<01:33,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     4] loss: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:23<02:54, 12.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 0.704\n",
      "[1,     6] loss: 0.749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:31<01:31,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     7] loss: 0.677\n",
      "[1,     8] loss: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [02:22<02:26, 14.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     9] loss: 0.668\n",
      "[1,    10] loss: 0.756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [02:27<01:05,  8.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    11] loss: 0.688\n",
      "[1,    12] loss: 0.797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [03:39<01:54, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    13] loss: 0.695\n",
      "[1,    14] loss: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [03:40<00:38,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    15] loss: 0.703\n",
      "[1,    16] loss: 0.657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [04:38<00:33, 16.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    17] loss: 0.768\n",
      "[1,    18] loss: 0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [04:38<00:00, 13.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    19] loss: 0.724\n",
      "[1,    20] loss: 0.560\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [01:11<01:58, 29.72s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Training Loop\n",
    "best_loss = 1e10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "\n",
    "    since = time.time()\n",
    "    model.train()\n",
    "    \n",
    "    metrics = defaultdict(float)\n",
    "    epoch_samples = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    print(f\"Epoch {epoch} - Training...\\n\")\n",
    "    # for inputs, labels in dataloaders[phase]:\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        inputs, target = batch['image'], batch['target']\n",
    "        \n",
    "        # mask = mask.mean(1,keepdim=True)  #transform from [batch_size, 3, size, size] to [batch,size, 1, size, size]\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        target = target.to(device)\n",
    "        # mask = mask.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # track history if only in train\n",
    "        # with torch.set_grad_enabled(phase == 'train'):\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # loss = calc_loss(outputs, mask, metrics, bce_weight=1)\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        epoch_samples += inputs.size(0)\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.3f}')\n",
    "        # running_loss = 0.0\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f'Train Acc: {100 * correct // total} %')\n",
    "    \n",
    "    #test\n",
    "    print(\"Testing...\")\n",
    "    test_model(model, test_loader)\n",
    "    \n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "# torch.save(model.state_dict(), \"trained_model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7b555b-b7cf-4ddc-8345-ad5775736078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
