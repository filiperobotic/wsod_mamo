{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d589928-22d6-4abf-9915-37b8d68929af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "# from keras.backend.common import _EPSILON\n",
    "# from tensorflow.keras.backend import common \n",
    "\n",
    "# Misc\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "import jsonref\n",
    "import json\n",
    "\n",
    "# Save\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For custom weighted loss\n",
    "# from tensorflow.keras.backend import _to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ec3ec6-3603-41ee-9c03-f2bed4935bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ac45f8-0801-4b27-b261-200a73729772",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_img_dir = \"/datasets/mamografia/CBIS-DDSM_organized/images/preprocessed/train/full\"\n",
    "train_mask_img_dir = \"/datasets/mamografia/CBIS-DDSM_organized/images/preprocessed/train/merged_masks\"\n",
    "test_full_img_dir = \"/datasets/mamografia/CBIS-DDSM_organized/images/preprocessed/test/full\"\n",
    "test_mask_img_dir = \"/datasets/mamografia/CBIS-DDSM_organized/images/preprocessed/test/merged_masks\"\n",
    "extension = \".png\"\n",
    "valtest_split = 0.5\n",
    "\n",
    "train_full_img_paths = []\n",
    "train_mask_img_paths = []\n",
    "test_full_img_paths = []\n",
    "test_mask_img_paths = []\n",
    "\n",
    "# Get paths of train images and masks.\n",
    "for full in os.listdir(train_full_img_dir):\n",
    "    if full.endswith(extension):\n",
    "        train_full_img_paths.append(os.path.join(train_full_img_dir, full))\n",
    "\n",
    "for mask in os.listdir(test_full_img_dir):\n",
    "    if mask.endswith(extension):\n",
    "        train_mask_img_paths.append(os.path.join(train_mask_img_dir, mask))\n",
    "\n",
    "# Get paths of test images and masks.\n",
    "for full in os.listdir(test_full_img_dir):\n",
    "    if full.endswith(extension):\n",
    "        test_full_img_paths.append(os.path.join(test_full_img_dir, full))\n",
    "\n",
    "for mask in os.listdir(test_mask_img_dir):\n",
    "    if mask.endswith(extension):\n",
    "        test_mask_img_paths.append(os.path.join(test_mask_img_dir, mask))\n",
    "\n",
    "# Sort so that FULL and MASK images are in an order that corresponds\n",
    "# with each other.\n",
    "train_full_img_paths.sort()\n",
    "train_mask_img_paths.sort()\n",
    "test_full_img_paths.sort()\n",
    "test_mask_img_paths.sort()\n",
    "\n",
    "# Split test paths into validation and test sets.\n",
    "valid_x, test_x = train_test_split(test_full_img_paths, test_size=valtest_split, random_state=42)\n",
    "valid_y, test_y = train_test_split(test_mask_img_paths, test_size=valtest_split, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6daab1-69fd-4a75-9bf5-1e1903371085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder():\n",
    "        \n",
    "        # =========\n",
    "        #  Encoder\n",
    "        # =========\n",
    "        \n",
    "        # Get base model\n",
    "        VGG16_ = keras.applications.VGG16(include_top=False,\n",
    "                                          weights=\"imagenet\",\n",
    "                                          input_shape=self.encoder_input_shape)\n",
    "        \n",
    "        # Get list of layer names for skip connections later\n",
    "        layer_names = [layer.name for layer in VGG16_.layers]\n",
    "\n",
    "        # Get layer outputs\n",
    "        all_layer_outputs = [VGG16_.get_layer(layer_name).output for layer_name in layer_names]\n",
    "        \n",
    "        # Create encoder model\n",
    "        encoder_model = keras.Model(inputs=VGG16_.input, outputs=all_layer_outputs)\n",
    "        \n",
    "        # Freeze layers\n",
    "        encoder_model.trainable = False\n",
    "        \n",
    "        return encoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3528691-f9ff-4d1f-98d1-cf986c8976f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = build_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d742838e-5b51-426a-bce8-9f95ca727754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetVGG16:\n",
    "    \n",
    "    def __init__(self, encoder_input_shape, learning_rate, batch_size, kernel_size, decoder_strides, decoder_padding, decoder_activation):\n",
    "        \n",
    "        self.encoder_input_shape = encoder_input_shape\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.decoder_strides = decoder_strides\n",
    "        self.decoder_padding = decoder_padding\n",
    "        self.decoder_activation = decoder_activation\n",
    "        \n",
    "        \n",
    "    def build_encoder(self):\n",
    "        \n",
    "        # =========\n",
    "        #  Encoder\n",
    "        # =========\n",
    "        \n",
    "        # Get base model\n",
    "        VGG16_ = keras.applications.VGG16(include_top=False,\n",
    "                                          weights=\"imagenet\",\n",
    "                                          input_shape=self.encoder_input_shape)\n",
    "        \n",
    "        # Get list of layer names for skip connections later\n",
    "        layer_names = [layer.name for layer in VGG16_.layers]\n",
    "\n",
    "        # Get layer outputs\n",
    "        all_layer_outputs = [VGG16_.get_layer(layer_name).output for layer_name in layer_names]\n",
    "        \n",
    "        # Create encoder model\n",
    "        encoder_model = keras.Model(inputs=VGG16_.input, outputs=all_layer_outputs)\n",
    "        \n",
    "        # Freeze layers\n",
    "        encoder_model.trainable = False\n",
    "        \n",
    "        return encoder_model\n",
    "        \n",
    "    \n",
    "    def build_unet(self):\n",
    "        \n",
    "        # =============\n",
    "        #  Input layer\n",
    "        # =============\n",
    "        \n",
    "        unet_input = keras.Input(shape=self.encoder_input_shape, name=\"unet_input_layer\")\n",
    "        \n",
    "        \n",
    "        # =========\n",
    "        #  Encoder\n",
    "        # =========\n",
    "        \n",
    "        encoder_model = self.build_encoder()\n",
    "        all_encoder_layer_outputs = encoder_model(unet_input)\n",
    "        \n",
    "        # Get final encoder output (this will be the input for the decoder)\n",
    "        encoded_img = all_encoder_layer_outputs[-1]\n",
    "        \n",
    "        # Get outputs to be used for skip connections\n",
    "        # (I know the specific layers to be used for skip connections)\n",
    "        skip_outputs = [all_encoder_layer_outputs[i] for i in [2, 5, 9, 13, 17]]\n",
    "        \n",
    "        \n",
    "        # =========\n",
    "        #  Decoder\n",
    "        # =========\n",
    "        \n",
    "        decoder_filters = encoded_img.shape[-1]\n",
    "        \n",
    "#         # Input layer into decoder\n",
    "#         decoder_input = keras.Input(shape=encoded_img.shape[1:], name=\"encoded_img\")\n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 5 - `encoded_img` as initial input for decoder\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block5_up_convT\",\n",
    "                                          filters=decoder_filters,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(encoded_img)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block5_up_concat\", axis=-1)([x, skip_outputs[4]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block5_up_conv3\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block5_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block5_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 4\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block4_up_convT\",\n",
    "                                          filters=decoder_filters,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block4_up_concat\", axis=-1)([x, skip_outputs[3]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block4_up_conv3\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block4_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block4_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 3\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block3_up_convT\",\n",
    "                                          filters=decoder_filters / 2,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block3_up_concat\", axis=-1)([x, skip_outputs[2]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block3_up_conv3\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block3_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block3_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 2\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block2_up_convT\",\n",
    "                                          filters=decoder_filters / 4,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block2_up_concat\", axis=-1)([x, skip_outputs[1]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block2_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block2_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Block 1\n",
    "        x = keras.layers.Conv2DTranspose(name=\"block1_up_convT\",\n",
    "                                          filters=decoder_filters / 4,\n",
    "                                          kernel_size=self.kernel_size,\n",
    "                                          strides=self.decoder_strides,\n",
    "                                          padding=self.decoder_padding,\n",
    "                                          activation=self.decoder_activation)(x)\n",
    "        \n",
    "        x = keras.layers.Concatenate(name=\"block1_up_concat\", axis=-1)([x, skip_outputs[0]])\n",
    "        \n",
    "        x = keras.layers.Conv2D(name=\"block1_up_conv2\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        x = keras.layers.Conv2D(name=\"block1_up_conv1\",\n",
    "                                filters=decoder_filters,\n",
    "                                kernel_size=self.kernel_size,\n",
    "                                strides=(1, 1),\n",
    "                                padding=\"same\",\n",
    "                                activation=\"relu\")(x)\n",
    "        \n",
    "        \n",
    "        # ------------------------------------------\n",
    "        # Final conv layer\n",
    "        final_img = keras.layers.Conv2D(name=\"final_up_conv\",\n",
    "                                        filters=3,\n",
    "                                        kernel_size=self.kernel_size,\n",
    "                                        strides=(1, 1),\n",
    "                                        padding=\"same\",\n",
    "                                        activation=\"relu\")(x)\n",
    "        \n",
    "        # ======\n",
    "        #  Unet\n",
    "        # ======\n",
    "        \n",
    "        unet = keras.Model(inputs=unet_input, outputs=final_img, name=\"Unet_VGG16\")\n",
    "        \n",
    "        return unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "589d766d-fec7-4414-937d-859f8ed9f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UnetVGG16(encoder_input_shape=(448, 448, 3),\n",
    "                 learning_rate=0.001,\n",
    "                 batch_size=1,\n",
    "                 kernel_size=(3, 3),\n",
    "                 decoder_strides=(2, 2),\n",
    "                 decoder_padding=\"same\",\n",
    "                 decoder_activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37cd0887-f1db-439e-a6c7-cc0f07e7a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = unet.build_unet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2620bf04-891c-4f2b-8023-b664610fbd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet_VGG16\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "unet_input_layer (InputLayer)   [(None, 448, 448, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model (Functional)              [(None, 448, 448, 3) 14714688    unet_input_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_convT (Conv2DTranspos (None, 28, 28, 512)  2359808     model[0][18]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_concat (Concatenate)  (None, 28, 28, 1024) 0           block5_up_convT[0][0]            \n",
      "                                                                 model[0][17]                     \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_conv3 (Conv2D)        (None, 28, 28, 512)  4719104     block5_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_conv2 (Conv2D)        (None, 28, 28, 512)  2359808     block5_up_conv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_up_conv1 (Conv2D)        (None, 28, 28, 512)  2359808     block5_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_convT (Conv2DTranspos (None, 56, 56, 512)  2359808     block5_up_conv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_concat (Concatenate)  (None, 56, 56, 1024) 0           block4_up_convT[0][0]            \n",
      "                                                                 model[0][13]                     \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_conv3 (Conv2D)        (None, 56, 56, 512)  4719104     block4_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_conv2 (Conv2D)        (None, 56, 56, 512)  2359808     block4_up_conv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_up_conv1 (Conv2D)        (None, 56, 56, 512)  2359808     block4_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_convT (Conv2DTranspos (None, 112, 112, 256 1179904     block4_up_conv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_concat (Concatenate)  (None, 112, 112, 512 0           block3_up_convT[0][0]            \n",
      "                                                                 model[0][9]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_conv3 (Conv2D)        (None, 112, 112, 512 2359808     block3_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_conv2 (Conv2D)        (None, 112, 112, 512 2359808     block3_up_conv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_up_conv1 (Conv2D)        (None, 112, 112, 512 2359808     block3_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_up_convT (Conv2DTranspos (None, 224, 224, 128 589952      block3_up_conv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_up_concat (Concatenate)  (None, 224, 224, 256 0           block2_up_convT[0][0]            \n",
      "                                                                 model[0][5]                      \n",
      "__________________________________________________________________________________________________\n",
      "block2_up_conv2 (Conv2D)        (None, 224, 224, 512 1180160     block2_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_up_conv1 (Conv2D)        (None, 224, 224, 512 2359808     block2_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_up_convT (Conv2DTranspos (None, 448, 448, 128 589952      block2_up_conv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_up_concat (Concatenate)  (None, 448, 448, 192 0           block1_up_convT[0][0]            \n",
      "                                                                 model[0][2]                      \n",
      "__________________________________________________________________________________________________\n",
      "block1_up_conv2 (Conv2D)        (None, 448, 448, 512 885248      block1_up_concat[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_up_conv1 (Conv2D)        (None, 448, 448, 512 2359808     block1_up_conv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "final_up_conv (Conv2D)          (None, 448, 448, 3)  13827       block1_up_conv1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 54,549,827\n",
      "Trainable params: 39,835,139\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d258a7fe-5a29-47a9-8262-82fbe6f0cb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(unet_model, to_file=\"./unet_model_v1.png\", show_shapes=True, show_layer_names=True, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e271994-c4c5-4568-9e00-8547f264a50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model.compile(optimizer=keras.optimizers.Adam(),\n",
    "                   loss=\"binary_crossentropy\",\n",
    "                   metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf531a0e-0f2c-40ab-aadb-da080f1cd2e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GenerateInputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d69619c6f3c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGenerateInputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'GenerateInputs' is not defined"
     ]
    }
   ],
   "source": [
    "unet_model.fit(GenerateInputs(X=x_train, y=y_train), batch_size=1, steps_per_epoch = 5, epochs=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cca77-eef6-410c-9e56-da66d2bb57ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
